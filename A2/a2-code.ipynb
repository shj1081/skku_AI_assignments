{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrdLnnGogSti"
      },
      "source": [
        "# Importing libraries / checking GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RfIqrK6Ngz-",
        "outputId": "17e60ee5-5b5b-41a5-cef9-7077399a1e3c"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e2QKqSHnRVSc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # For visualizations\n",
        "import numpy as np # For numerical computations\n",
        "import tensorflow as tf # For machine learning models\n",
        "import tensorflow_datasets as tfds # For loading datasets\n",
        "import datetime # For date and time manipulation\n",
        "from tensorflow.keras import layers, Model, models # For building Keras models\n",
        "from tensorflow.keras.applications import EfficientNetB0 # Pre-trained model for image classification\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # For confusion matrix\n",
        "\n",
        "tf.random.set_seed(42) # For reproducibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylBpinj2gStj"
      },
      "source": [
        "# Load EuroSAT dataset with splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pz_c6QmQRVnw"
      },
      "outputs": [],
      "source": [
        "# Define dataset splits\n",
        "splits = (\"train[:70%]\", \"train[70%:90%]\", \"train[90%:]\")\n",
        "\n",
        "# Load EuroSAT dataset with splits and metadata\n",
        "(train, val, test), metadata = tfds.load(\n",
        "    name=\"eurosat\",  # Dataset name\n",
        "    as_supervised=True,  # Load as (image, label) pairs\n",
        "    split=splits,  # Training, validation, and test splits\n",
        "    with_info=True  # Load dataset metadata\n",
        ")\n",
        "\n",
        "# Convert label indices to string labels\n",
        "get_label_name = metadata.features[\"label\"].int2str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z9Lu1VBgStk"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bsJPLyDtRXyg"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "batch_size = 32  # Samples per update\n",
        "epochs = 10  # Training epochs\n",
        "img_height = 64  # Image height\n",
        "img_width = 64  # Image width\n",
        "num_classes = metadata.features[\"label\"].num_classes  # Unique class count\n",
        "\n",
        "# Function to resize and normalize images\n",
        "def resize_and_rescale(image, label):\n",
        "    image = tf.cast(image, tf.float32)  # Convert to float32\n",
        "    image = tf.image.resize(image, [img_height, img_width])  # Resize image\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image, label  # Return image and label\n",
        "\n",
        "# Function to augment training images\n",
        "def augment(image, label):\n",
        "    image, label = resize_and_rescale(image, label)  # Resize and normalize\n",
        "    image = tf.image.rot90(image)  # Rotate image 90 degrees\n",
        "    image = tf.image.random_crop(image, size=[img_height, img_width, 3])  # Random crop\n",
        "    return image, label  # Return augmented image and label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KCy2MOrHRZ8x"
      },
      "outputs": [],
      "source": [
        "# Set auto-tuning for data pipeline\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Shuffle, augment, batch, and prefetch the training dataset\n",
        "train = (\n",
        "    train.shuffle(1000)  # Shuffle the training data\n",
        "    .map(augment, num_parallel_calls=AUTOTUNE)  # Apply augmentation\n",
        "    .batch(batch_size)  # Batch the data\n",
        "    .prefetch(AUTOTUNE)  # Prefetch for performance optimization\n",
        ")\n",
        "\n",
        "# Resize and batch the validation dataset\n",
        "val = (\n",
        "    val.map(resize_and_rescale, num_parallel_calls=AUTOTUNE)  # Resize and normalize\n",
        "    .batch(batch_size)  # Batch the data\n",
        "    .prefetch(AUTOTUNE)  # Prefetch for performance optimization\n",
        ")\n",
        "\n",
        "# Resize and batch the test dataset\n",
        "test = (\n",
        "    test.map(resize_and_rescale, num_parallel_calls=AUTOTUNE)  # Resize and normalize\n",
        "    .batch(batch_size)  # Batch the data\n",
        "    .prefetch(AUTOTUNE)  # Prefetch for performance optimization\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF9wFq6ogStl"
      },
      "source": [
        "# Load pre-trained EfficientNetB0 model with imagenet weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "Qax8CxnyRcC8",
        "outputId": "9f348743-7e63-46d1-c14f-c1a5957d6468"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained EfficientNetB0 model without the top layer\n",
        "base_model = EfficientNetB0(\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "\n",
        "# Build the complete model with additional classification layers\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(metadata.features['label'].num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model and show summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wXjK4CSgStl"
      },
      "source": [
        "# Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3N9E0ngRruU",
        "outputId": "5171de73-90a5-4690-c93a-9ed3d7a0d2a0"
      },
      "outputs": [],
      "source": [
        "# Set the base model to be trainable for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile the model using the Adam optimizer with specified loss and metrics\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Define the log directory for TensorBoard\n",
        "log_dir = \"logs\" + datetime.datetime.now().strftime(\"/%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train the model using the training and validation datasets\n",
        "history_fine = model.fit(\n",
        "    train,  # Training data\n",
        "    validation_data=val,  # Validation data\n",
        "    epochs=20,  # Total epochs for training\n",
        "    callbacks=[tensorboard_callback],  # Save best weights during training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0dkM5lKgStl"
      },
      "source": [
        "# Evaluate the fine-tuned model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dTsUWEzSiiD",
        "outputId": "e3a3e7f6-2861-4329-cc00-cbfcc1940da3"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3AcivtNSSzMF",
        "outputId": "a3ad4131-1868-4f39-96be-dfeab2e9b2aa"
      },
      "outputs": [],
      "source": [
        "# Get predictions for the entire test dataset\n",
        "y_pred = model.predict(test)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Get true labels for the entire test dataset\n",
        "y_true = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=metadata.features['label'].names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
